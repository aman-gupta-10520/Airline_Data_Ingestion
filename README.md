Process and Ingest the daily flight data into the Redshift data warehouse fact table.
In this project we are receiving the daily flight data which needs to be loaded into the Redshift tables.
To achieve this we have used below AWS services.

High Level of AWS Services Usage: As soon as we receive the file in our S3 bucket from the upstream system, CloudTrail event will trigger the API call to EventBridge where we have created a Rule to filter our targated changes, once this call satisfy our eventbridge rule, step fucntion will be triggered whihc will run the necessary crawlers following by Glue job.
This glue job will transform our data and load to redshift table, once the job is succesfull step function will trigger the SNS notification to our mail id.

# Tech Stack


### Extract:

S3, Glue Crawler

### Transform:

Glue Low Code ETL

### Load:

Redshift

### Alerting:

Event Bridge, SNS, S3 CloudTrail Notification

### Orchestration:

Step Function

# S3

Directory Structure:
![image](https://github.com/user-attachments/assets/17814da0-6399-480b-8cc2-c0fadd0560b0)

Create a crawler for the S3 input file:

![image](https://github.com/user-attachments/assets/9dfe2cae-1164-45ee-b892-74b70cca7073)


Run the above crawler and check the catalog table created for the S3 input file - daily_raw_data:

![image](https://github.com/user-attachments/assets/a91eb6d7-12aa-431b-9ed6-d6487ea2ccf8)

![image](https://github.com/user-attachments/assets/35b98f1f-f644-4def-b533-7bf2a020e6ea)


# Redshift

Now create the dimenstion and fact tables in redshift using the commands redshift_create_table_commands.txt
Fact table (airlines.daily_flights_fact ) will be loaded by our job but the dimenstion table (airlines.airports_dim) we will load manually using the copy command present in above file.

Create JDBC connection for Glue crawler to get the data catalog tables for fact and dimenstion tables present in Redshift:

![image](https://github.com/user-attachments/assets/8dd8e2d4-b158-4d0c-8d31-dea9b9cbd7f2)

Once the tables are created in Redshift we can create the crawlers and run them to get the schema:

![image](https://github.com/user-attachments/assets/0109c753-cbb1-4668-9472-ddfef4e83adb)

![image](https://github.com/user-attachments/assets/71b6328f-b49b-4c0c-a843-48510c77c53f)


# Glue Low Code ETL

![image](https://github.com/user-attachments/assets/31821c88-8074-491d-9eb2-66d367ed9d4b)

attached python file:- airline_data_ingestion.py

This job is used to update the daily raw file data to redshift fact table which is daily_flights_fact by getting the meta data present in dimestion table (airports_dim).
In the code we are getting the data from redshift for dimenstion table (airports_dim) and daily raw file data from s3.
after that we are joining the both tables and after transforming the columns of the joined table again we are joining this table with dimenstion table (airports_dim) as shown in above visuals.
first join was on the basis of sourc airport_id and the other join is based on destination airport_id.
once the data is joined we will again tranformed the data as per the structure of fact table in redshift. And after transforming we will load the final data into the fact table (daily_flights_fact) in Redshift.


# SNS

We will create a SNS topic which will trigger the mail to the mentioned email id for the job failure or job completion as below:

![image](https://github.com/user-attachments/assets/34ba3db4-213a-45b7-a68a-20399c399a22)

![image](https://github.com/user-attachments/assets/42c9ddf2-8a83-40d4-8093-845b432e3f6e)

# Cloud Trail

CloudTrail is a service that records AWS API calls and user activity within an AWS account. When an object in an S3 bucket is created, modified, or deleted, CloudTrail records this as an event.
This CLoud Trail will capture any changes in the S3 object and in our case whenever daily flight raw data file will be uploaded, it will create the api call to eventbridge where we will done furthure filteration.
Here we will create the a cloud trail object as shown below:

![image](https://github.com/user-attachments/assets/d7fba21a-be12-4716-ab1a-de777efda3f6)

And will add proper IAM role as below:

![image](https://github.com/user-attachments/assets/c673f8c2-de90-418e-a183-4fba8ec789ff)


Once it is create we will attache this cloud trail object to our S3 object under the properties tab for airline data as below:

![image](https://github.com/user-attachments/assets/f4d71cc8-473b-464a-a8b4-a9f53a391b88)

# EventBridge

EventBridge can capture CloudTrail events in real time and use rules to filter specific events
Once there is any API call generated by cloudTrail, it will capture and filter it with the help of the rule which will be created as shown below:

![image](https://github.com/user-attachments/assets/4d285b18-5852-4e71-b6e6-155ee8ea4ced)

![image](https://github.com/user-attachments/assets/4c0a8422-b8fd-4fea-abb4-e25846429350)

Once the trigger it verified by this Rule, step function will be triggred whihc we will create in next steps.


# Step Function

So once there is a change in S3 object and it get verify by the EventBridge RUle created in above step, step funciton will be triggered and it will furthure run the crawlers and the glue job.
and after that it will trigger the notification to our mail id for job completion or failure.

![image](https://github.com/user-attachments/assets/d68f77f5-9da0-4618-9d11-904f2bf32849)

![image](https://github.com/user-attachments/assets/1cd4f58a-578e-4c10-9712-6580f51eb7bc)


